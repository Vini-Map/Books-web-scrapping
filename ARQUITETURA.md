# ğŸ—ï¸ Plano Arquitetural â€” Books Web Scraping & API

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral da Arquitetura Atual](#1-visÃ£o-geral-da-arquitetura-atual)
2. [Pipeline de Dados Detalhado](#2-pipeline-de-dados-detalhado)
3. [Arquitetura da API](#3-arquitetura-da-api)
4. [Escalabilidade Futura](#4-escalabilidade-futura)
5. [CenÃ¡rios de Uso para Cientistas de Dados/ML](#5-cenÃ¡rios-de-uso-para-cientistas-de-dadosml)
6. [Plano de IntegraÃ§Ã£o com Modelos de ML](#6-plano-de-integraÃ§Ã£o-com-modelos-de-ml)

---

## 1. VisÃ£o Geral da Arquitetura Atual

### 1.1 Diagrama de Alto NÃ­vel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Books to Scrape    â”‚  â† Fonte de Dados Externa
â”‚  (Site Web)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTP Requests
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SCRAPER           â”‚
â”‚   (BeautifulSoup)   â”‚  â† scripts/scraper.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ CSV Raw
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   data/books.csv    â”‚  â† Armazenamento Raw
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Read CSV
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA PROCESSOR      â”‚
â”‚ (Pandas)            â”‚  â† scripts/process_data.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ CSV Processed
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ data/processed/     â”‚  â† Dados Limpos
â”‚ books_clean.csv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Load & Cache
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FASTAPI           â”‚
â”‚   REST API          â”‚  â† api/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ JSON via HTTP
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CONSUMIDORES      â”‚
â”‚   â€¢ Web Apps        â”‚
â”‚   â€¢ Data Scientists â”‚
â”‚   â€¢ ML Engineers    â”‚
â”‚   â€¢ BI Tools        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Arquitetura em Camadas

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              CAMADA DE APRESENTAÃ‡ÃƒO                   â•‘
â•‘  â€¢ Swagger UI (DocumentaÃ§Ã£o Interativa)              â•‘
â•‘  â€¢ ReDoc (DocumentaÃ§Ã£o Alternativa)                  â•‘
â•‘  â€¢ JSON Responses                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â–²
                        â”‚
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              CAMADA DE APLICAÃ‡ÃƒO (API)                â•‘
â•‘  FastAPI Framework:                                   â•‘
â•‘  â€¢ api/main.py        - AplicaÃ§Ã£o principal          â•‘
â•‘  â€¢ api/routes/        - Endpoints organizados        â•‘
â•‘  â€¢ api/models.py      - Schemas Pydantic             â•‘
â•‘  â€¢ api/utils.py       - FunÃ§Ãµes auxiliares           â•‘
â•‘  â€¢ api/data_loader.py - Carregamento de dados        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â–²
                        â”‚
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           CAMADA DE ARMAZENAMENTO                     â•‘
â•‘  â€¢ data/books.csv         - Dados brutos             â•‘
â•‘  â€¢ data/processed/        - Dados processados        â•‘
â•‘  â€¢ Cache em memÃ³ria       - DataFrame carregado      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â–²
                        â”‚
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            CAMADA DE PROCESSAMENTO                    â•‘
â•‘  â€¢ scripts/process_data.py - Limpeza e validaÃ§Ã£o    â•‘
â•‘  â€¢ Pandas transformations  - ETL                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â–²
                        â”‚
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             CAMADA DE INGESTÃƒO                        â•‘
â•‘  â€¢ scripts/scraper.py     - Web scraping             â•‘
â•‘  â€¢ BeautifulSoup4         - HTML parsing             â•‘
â•‘  â€¢ Requests               - HTTP client              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 2. Pipeline de Dados Detalhado

### 2.1 Fase 1: INGESTÃƒO - Web Scraping

**Arquivo:** `scripts/scraper.py`

#### Fluxo de ExecuÃ§Ã£o

```
INÃCIO
  â”‚
  â”œâ”€â–º 1. Inicializar sessÃ£o HTTP
  â”‚   â””â”€â–º requests.Session()
  â”‚
  â”œâ”€â–º 2. Loop pelas pÃ¡ginas do catÃ¡logo
  â”‚   â”œâ”€â–º GET /catalogue/page-1.html
  â”‚   â”œâ”€â–º Parse HTML com BeautifulSoup
  â”‚   â”œâ”€â–º Identificar cards de livros
  â”‚   â””â”€â–º Para cada livro:
  â”‚       â”œâ”€â–º Extrair link do produto
  â”‚       â”œâ”€â–º GET pÃ¡gina individual do livro
  â”‚       â”œâ”€â–º Extrair dados (parse_book)
  â”‚       â”‚   â”œâ”€â–º TÃ­tulo (tag h1)
  â”‚       â”‚   â”œâ”€â–º PreÃ§o (class price_color)
  â”‚       â”‚   â”œâ”€â–º Estoque (class instock)
  â”‚       â”‚   â”œâ”€â–º Rating (class star-rating)
  â”‚       â”‚   â”œâ”€â–º Categoria (breadcrumb)
  â”‚       â”‚   â”œâ”€â–º UPC (tabela product info)
  â”‚       â”‚   â””â”€â–º DescriÃ§Ã£o (tag p apÃ³s product_description)
  â”‚       â””â”€â–º Adicionar Ã  lista de resultados
  â”‚
  â”œâ”€â–º 3. Navegar para prÃ³xima pÃ¡gina
  â”‚   â””â”€â–º Verificar botÃ£o "next"
  â”‚
  â”œâ”€â–º 4. Consolidar dados
  â”‚   â””â”€â–º Criar lista com todos os livros
  â”‚
  â””â”€â–º 5. Salvar em CSV
      â””â”€â–º data/books.csv (UTF-8)
FIM
```

#### Dados Coletados

| Campo | Origem no HTML | Tipo | Exemplo |
|-------|---------------|------|---------|
| `id` | Gerado sequencialmente | int | 1 |
| `title` | `<h1>` | str | "A Light in the Attic" |
| `price` | `<p class="price_color">` | str | "Â£51.77" |
| `stock` | `<p class="instock availability">` | str | "In stock (22 available)" |
| `rating` | `<p class="star-rating Three">` | int | 3 |
| `category` | `<ul class="breadcrumb">` > li[2] | str | "Poetry" |
| `product_page_url` | URL construÃ­da | str | "https://books.toscrape..." |
| `upc` | `<table>` > tr com th="UPC" | str | "a897fe39b1053632" |
| `description` | `<p>` apÃ³s `#product_description` | str | "It's hard to imagine..." |

#### CaracterÃ­sticas TÃ©cnicas

```python
# Rate Limiting
time.sleep(0.5)  # Delay entre requests

# Error Handling por livro
try:
    item = parse_book(book_url, session)
except Exception as e:
    print("  ! error parsing:", e)
    # Continua para o prÃ³ximo livro

# Timeout configurado
session.get(url, timeout=10)

# NormalizaÃ§Ã£o de URLs relativas
book_url = urljoin(url, href).replace('../', '')
```

#### Qualidade da ExtraÃ§Ã£o

- âœ… **Completude**: Extrai todos os 9 campos disponÃ­veis
- âœ… **Robustez**: Try/catch individual por livro
- âœ… **ResiliÃªncia**: NÃ£o interrompe scraping por falhas pontuais
- âœ… **Performance**: Session reutilizada, delay controlado
- âš ï¸ **LimitaÃ§Ã£o**: Single-threaded (sequencial)

---

### 2.2 Fase 2: PROCESSAMENTO - Limpeza de Dados

**Arquivo:** `scripts/process_data.py`

#### Pipeline de TransformaÃ§Ã£o

```
LEITURA
  â”‚
  â”œâ”€â–º pd.read_csv('data/books.csv')
  â”‚
  â–¼
VALIDAÃ‡ÃƒO DE COLUNAS
  â”‚
  â”œâ”€â–º Verificar presenÃ§a de campos obrigatÃ³rios
  â”œâ”€â–º Criar colunas ausentes com None
  â”‚
  â–¼
TRANSFORMAÃ‡Ã•ES
  â”‚
  â”œâ”€â–º 1. PRICE
  â”‚   â”œâ”€â–º Remover sÃ­mbolo Â£
  â”‚   â”œâ”€â–º Extrair nÃºmeros com regex
  â”‚   â”œâ”€â–º Converter para float
  â”‚   â”‚   "Â£51.77" â†’ 51.77
  â”‚
  â”œâ”€â–º 2. STOCK
  â”‚   â”œâ”€â–º Extrair nÃºmero com regex
  â”‚   â”œâ”€â–º Converter para int
  â”‚   â”‚   "In stock (22 available)" â†’ 22
  â”‚
  â”œâ”€â–º 3. TITLE
  â”‚   â”œâ”€â–º Converter para string
  â”‚   â”œâ”€â–º Remover espaÃ§os em branco
  â”‚   â”‚   "  A Light  " â†’ "A Light"
  â”‚
  â””â”€â–º 4. CATEGORY
      â”œâ”€â–º Converter para string
      â””â”€â–º Remover espaÃ§os em branco
  â”‚
  â–¼
PERSISTÃŠNCIA
  â”‚
  â””â”€â–º df.to_csv('data/processed/books_clean.csv')
```

#### FunÃ§Ãµes de Limpeza

```python
def parse_price(s):
    """
    Converte string de preÃ§o em float
    Entrada: "Â£51.77"
    SaÃ­da: 51.77
    """
    if pd.isna(s):
        return None
    s = str(s).replace('Â£','').replace('\n','').strip()
    m = re.search(r"\d+[.,]?\d*", s)
    if not m:
        return None
    return float(m.group(0).replace(',', '.'))

def parse_stock(s):
    """
    Extrai quantidade numÃ©rica de estoque
    Entrada: "In stock (22 available)"
    SaÃ­da: 22
    """
    if pd.isna(s):
        return 0
    m = re.search(r"(\d+)", str(s))
    return int(m.group(1)) if m else 0
```

#### MÃ©tricas de Qualidade

| Aspecto | ImplementaÃ§Ã£o | Resultado |
|---------|---------------|-----------|
| **Completude** | ValidaÃ§Ã£o de colunas | 100% de campos |
| **ConsistÃªncia** | ConversÃ£o forÃ§ada de tipos | Tipos uniformes |
| **PrecisÃ£o** | Regex para extraÃ§Ã£o | Valores corretos |
| **PadronizaÃ§Ã£o** | strip() em strings | Sem espaÃ§os extras |

---

### 2.3 Fase 3: API - ExposiÃ§Ã£o dos Dados

**Estrutura:** `api/`

#### Componentes da API

```
api/
â”‚
â”œâ”€â”€ main.py                 â† AplicaÃ§Ã£o FastAPI principal
â”‚   â”œâ”€â–º ConfiguraÃ§Ã£o CORS
â”‚   â”œâ”€â–º InclusÃ£o de routers
â”‚   â”œâ”€â–º CustomizaÃ§Ã£o Swagger
â”‚   â””â”€â–º InicializaÃ§Ã£o da app
â”‚
â”œâ”€â”€ data_loader.py          â† Gerenciamento de dados
â”‚   â”œâ”€â–º Carregamento do CSV
â”‚   â”œâ”€â–º Cache em memÃ³ria (DataFrame)
â”‚   â””â”€â–º FunÃ§Ãµes de acesso
â”‚
â”œâ”€â”€ models.py               â† Schemas Pydantic
â”‚   â”œâ”€â–º BookResponse
â”‚   â”œâ”€â–º BookList
â”‚   â””â”€â–º Modelos de validaÃ§Ã£o
â”‚
â”œâ”€â”€ utils.py                â† FunÃ§Ãµes auxiliares
â”‚   â””â”€â–º Helpers diversos
â”‚
â””â”€â”€ routes/                 â† Endpoints organizados
    â”œâ”€â”€ books.py            â† CRUD de livros
    â”œâ”€â”€ categories.py       â† OperaÃ§Ãµes com categorias
    â”œâ”€â”€ stats.py            â† EstatÃ­sticas
    â””â”€â”€ health.py           â† Health check
```

#### Endpoints Implementados

**1. Books (api/routes/books.py)**

```python
GET /api/v1/books
    ParÃ¢metros:
    - page: int = 1
    - size: int = 10
    - category: str | None
    - min_price: float | None
    - max_price: float | None
    - min_rating: int | None
    
    Retorna: Lista paginada de livros

GET /api/v1/books/{book_id}
    Retorna: Detalhes de um livro especÃ­fico

GET /api/v1/books/search
    ParÃ¢metros:
    - title: str | None
    - category: str | None
    
    Retorna: Livros que correspondem Ã  busca
```

**2. Categories (api/routes/categories.py)**

```python
GET /api/v1/categories
    Retorna: DicionÃ¡rio {categoria: quantidade}
```

**3. Stats (api/routes/stats.py)**

```python
GET /api/v1/stats/overview
    Retorna:
    - total_books: int
    - total_categories: int
    - average_price: float
    - average_rating: float
    - price_range: {min, max}

GET /api/v1/stats/categories
    Retorna: EstatÃ­sticas por categoria
```

**4. Health (api/routes/health.py)**

```python
GET /api/v1/health
    Retorna:
    - status: "healthy"
    - timestamp: datetime
    - data_loaded: bool
    - total_books: int
```

#### Fluxo de RequisiÃ§Ã£o

```
Cliente HTTP
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Routing    â”‚
â”‚  (Decorators @app)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ValidaÃ§Ã£o Pydantic â”‚
â”‚  (Request params)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LÃ³gica de NegÃ³cio  â”‚
â”‚  (Route handler)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Loader        â”‚
â”‚  (Acesso ao cache)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataFrame Pandas   â”‚
â”‚  (OperaÃ§Ãµes)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SerializaÃ§Ã£o JSON  â”‚
â”‚  (Response model)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    Response HTTP
```

#### Gerenciamento de Dados em MemÃ³ria

```python
# api/data_loader.py (Exemplo simplificado)

class DataLoader:
    def __init__(self):
        self._df = None
    
    def load_data(self):
        """Carrega CSV uma Ãºnica vez na inicializaÃ§Ã£o"""
        if self._df is None:
            self._df = pd.read_csv('data/books.csv')
    
    def get_all_books(self):
        """Retorna todos os livros"""
        return self._df.to_dict('records')
    
    def get_book_by_id(self, book_id: int):
        """Busca livro por ID"""
        book = self._df[self._df['id'] == book_id]
        return book.to_dict('records')[0] if not book.empty else None
    
    def filter_books(self, **filters):
        """Aplica filtros ao DataFrame"""
        df_filtered = self._df.copy()
        
        if 'category' in filters:
            df_filtered = df_filtered[
                df_filtered['category'] == filters['category']
            ]
        
        if 'min_price' in filters:
            df_filtered = df_filtered[
                df_filtered['price'] >= filters['min_price']
            ]
        
        # ... outros filtros
        
        return df_filtered.to_dict('records')

# InstÃ¢ncia global
data_loader = DataLoader()
```

#### CaracterÃ­sticas da API

| CaracterÃ­stica | ImplementaÃ§Ã£o | BenefÃ­cio |
|----------------|---------------|-----------|
| **DocumentaÃ§Ã£o AutomÃ¡tica** | Swagger + ReDoc | FÃ¡cil consumo |
| **ValidaÃ§Ã£o** | Pydantic models | Type safety |
| **CORS** | CORSMiddleware | Cross-origin |
| **PaginaÃ§Ã£o** | Query params | Performance |
| **Filtros** | Query params | Flexibilidade |
| **Cache** | DataFrame em memÃ³ria | Velocidade |
| **Rotas organizadas** | APIRouter | Manutenibilidade |

---

## 3. Arquitetura da API

### 3.1 Design Patterns Utilizados

#### 3.1.1 Repository Pattern (Simplificado)

```python
# data_loader.py age como um Repository
# Abstrai acesso aos dados do resto da aplicaÃ§Ã£o

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Routes     â”‚  â† NÃ£o conhecem detalhes de armazenamento
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Loader  â”‚  â† Interface de acesso aos dados
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV/Pandas  â”‚  â† ImplementaÃ§Ã£o de armazenamento
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.1.2 Router Pattern

```python
# OrganizaÃ§Ã£o modular por domÃ­nio

main.py
  â”œâ”€â–º books_router      (api/routes/books.py)
  â”œâ”€â–º categories_router (api/routes/categories.py)
  â”œâ”€â–º stats_router      (api/routes/stats.py)
  â””â”€â–º health_router     (api/routes/health.py)
```

### 3.2 Modelo de Dados

#### Schema Principal: Book

```python
class BookResponse(BaseModel):
    id: int
    title: str
    price: Optional[float] = None
    stock: Optional[int] = None
    rating: Optional[int] = None
    category: Optional[str] = None
    product_page_url: Optional[str] = None
    upc: Optional[str] = None
    description: Optional[str] = None
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": 1,
                "title": "A Light in the Attic",
                "price": 51.77,
                "stock": 22,
                "rating": 3,
                "category": "Poetry",
                "product_page_url": "https://books.toscrape.com/...",
                "upc": "a897fe39b1053632",
                "description": "It's hard to imagine..."
            }
        }
```

### 3.3 Fluxo de Dados Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. STARTUP                                         â”‚
â”‚     â€¢ FastAPI app criado                            â”‚
â”‚     â€¢ CORS configurado                              â”‚
â”‚     â€¢ Routers incluÃ­dos                             â”‚
â”‚     â€¢ DataLoader inicializado                       â”‚
â”‚     â€¢ CSV carregado em memÃ³ria                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. REQUEST                                         â”‚
â”‚     â€¢ Cliente faz HTTP GET /api/v1/books           â”‚
â”‚     â€¢ FastAPI valida query parameters              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ROUTING                                         â”‚
â”‚     â€¢ FastAPI router direciona para handler        â”‚
â”‚     â€¢ ParÃ¢metros extraÃ­dos e validados             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. PROCESSING                                      â”‚
â”‚     â€¢ Handler chama DataLoader                      â”‚
â”‚     â€¢ Filtros aplicados no DataFrame               â”‚
â”‚     â€¢ PaginaÃ§Ã£o aplicada                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. SERIALIZATION                                   â”‚
â”‚     â€¢ Dados convertidos para dicionÃ¡rios           â”‚
â”‚     â€¢ Pydantic valida/serializa response           â”‚
â”‚     â€¢ JSON gerado                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. RESPONSE                                        â”‚
â”‚     â€¢ HTTP 200 OK                                   â”‚
â”‚     â€¢ Headers (Content-Type: application/json)     â”‚
â”‚     â€¢ Body com lista de livros                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Escalabilidade Futura

### 4.1 LimitaÃ§Ãµes Atuais

| LimitaÃ§Ã£o | Impacto | Quando se torna problema |
|-----------|---------|--------------------------|
| **Dados em CSV** | Performance de leitura | > 100k registros |
| **Cache em memÃ³ria** | NÃ£o compartilhado entre instÃ¢ncias | Deploy multi-instÃ¢ncia |
| **Single instance** | Limite de throughput | > 100 req/s |
| **Scraping manual** | Dados desatualizados | Necessidade de atualizaÃ§Ã£o frequente |

### 4.2 PrÃ³ximos Passos de EvoluÃ§Ã£o

#### Fase 1: Banco de Dados (Curto Prazo)

```
Atual:                      Proposta:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV    â”‚   â”€â”€â”€â”€â”€â”€â–º     â”‚PostgreSQLâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BenefÃ­cios:
âœ“ Queries mais eficientes
âœ“ Ãndices para performance
âœ“ Suporte a concorrÃªncia
âœ“ Backup automatizado
```

**ImplementaÃ§Ã£o sugerida:**

```python
# MigraÃ§Ã£o simples com SQLAlchemy
from sqlalchemy import create_engine
import pandas as pd

# Carregar CSV
df = pd.read_csv('data/books.csv')

# Criar conexÃ£o PostgreSQL
engine = create_engine('postgresql://user:pass@localhost/books')

# Migrar dados
df.to_sql('books', engine, if_exists='replace', index=False)
```

#### Fase 2: Cache DistribuÃ­do (MÃ©dio Prazo)

```
Atual:                      Proposta:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ In-Memoryâ”‚   â”€â”€â”€â”€â”€â”€â–º     â”‚  Redis   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BenefÃ­cios:
âœ“ Cache compartilhado
âœ“ Suporte a mÃºltiplas instÃ¢ncias
âœ“ TTL configurÃ¡vel
âœ“ Performance consistente
```

#### Fase 3: OrquestraÃ§Ã£o (Longo Prazo)

```
Atual:                      Proposta:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Manual  â”‚   â”€â”€â”€â”€â”€â”€â–º     â”‚ Airflow  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BenefÃ­cios:
âœ“ Scraping agendado
âœ“ Pipeline automatizado
âœ“ Monitoramento
âœ“ Retry automÃ¡tico
```

### 4.3 Arquitetura EscalÃ¡vel (VisÃ£o Futura)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Load Balancerâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ API #1   â”‚              â”‚ API #2   â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚                         â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Redis    â”‚
                 â”‚   Cache    â”‚
                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ PostgreSQL  â”‚
                â”‚  Database   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. CenÃ¡rios de Uso para Cientistas de Dados/ML

### 5.1 AnÃ¡lise ExploratÃ³ria de Dados (EDA)

```python
import requests
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Conectar Ã  API
API_URL = "http://localhost:8000/api/v1"

# Carregar todos os livros
response = requests.get(f"{API_URL}/books", params={"size": 1000})
books = pd.DataFrame(response.json())

# AnÃ¡lises possÃ­veis:

# 1. DistribuiÃ§Ã£o de preÃ§os por categoria
plt.figure(figsize=(12, 6))
books.groupby('category')['price'].mean().sort_values().plot(kind='barh')
plt.title('PreÃ§o MÃ©dio por Categoria')
plt.xlabel('PreÃ§o (Â£)')
plt.tight_layout()
plt.show()

# 2. CorrelaÃ§Ã£o entre rating e preÃ§o
sns.scatterplot(data=books, x='price', y='rating', hue='category')
plt.title('RelaÃ§Ã£o entre PreÃ§o e Rating')
plt.show()

# 3. AnÃ¡lise de estoque
stock_analysis = books.groupby('category').agg({
    'stock': ['sum', 'mean', 'median'],
    'id': 'count'
})
print(stock_analysis)

# 4. Top categorias
top_categories = books['category'].value_counts().head(10)
print(top_categories)
```

### 5.2 Feature Engineering

```python
# Criar features derivadas para ML

# 1. Faixa de preÃ§o
def price_category(price):
    if price < 20:
        return 'Barato'
    elif price < 40:
        return 'MÃ©dio'
    else:
        return 'Caro'

books['price_category'] = books['price'].apply(price_category)

# 2. Disponibilidade
books['in_stock'] = books['stock'] > 0
books['stock_level'] = pd.cut(
    books['stock'], 
    bins=[0, 5, 15, 50], 
    labels=['Baixo', 'MÃ©dio', 'Alto']
)

# 3. Rating binarizado
books['high_rating'] = (books['rating'] >= 4).astype(int)

# 4. Tamanho do tÃ­tulo
books['title_length'] = books['title'].str.len()

# 5. Tem descriÃ§Ã£o
books['has_description'] = books['description'].notna()

# Exportar para anÃ¡lise
books.to_csv('features_engenharia.csv', index=False)
```

### 5.3 IntegraÃ§Ã£o com Notebooks

```python
# notebook_template.ipynb

"""
Notebook Template para AnÃ¡lise de Livros
Conecta automaticamente Ã  API local
"""

import requests
import pandas as pd
from typing import Optional

class BooksAPI:
    def __init__(self, base_url: str = "http://localhost:8000/api/v1"):
        self.base_url = base_url
    
    def get_books(self, 
                  page: int = 1, 
                  size: int = 100,
                  category: Optional[str] = None,
                  min_price: Optional[float] = None) -> pd.DataFrame:
        """Busca livros com filtros"""
        params = {
            "page": page,
            "size": size
        }
        if category:
            params["category"] = category
        if min_price:
            params["min_price"] = min_price
            
        response = requests.get(f"{self.base_url}/books", params=params)
        response.raise_for_status()
        return pd.DataFrame(response.json())
    
    def get_stats(self) -> dict:
        """Busca estatÃ­sticas gerais"""
        response = requests.get(f"{self.base_url}/stats/overview")
        response.raise_for_status()
        return response.json()
    
    def get_categories(self) -> dict:
        """Lista todas as categorias"""
        response = requests.get(f"{self.base_url}/categories")
        response.raise_for_status()
        return response.json()

# Instanciar cliente
api = BooksAPI()

# Exemplo de uso
df = api.get_books(size=1000)
stats = api.get_stats()
categories = api.get_categories()

print(f"Total de livros carregados: {len(df)}")
print(f"EstatÃ­sticas gerais: {stats}")
print(f"Categorias disponÃ­veis: {len(categories)}")
```

### 5.4 Pipeline de Dados para ML

```python
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# 1. Carregar dados da API
api = BooksAPI()
df = api.get_books(size=10000)

# 2. PreparaÃ§Ã£o para ML
# Features numÃ©ricas
numeric_features = ['price', 'stock', 'rating']

# Encoding de categorias
le_category = LabelEncoder()
df['category_encoded'] = le_category.fit_transform(df['category'])

# Features derivadas
df['price_per_star'] = df['price'] / df['rating']
df['in_stock'] = (df['stock'] > 0).astype(int)

# 3. Split treino/teste
X = df[['price', 'stock', 'rating', 'category_encoded', 'price_per_star']]
y = df['in_stock']  # Exemplo: prever disponibilidade

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Dados de treino: {X_train.shape}")
print(f"Dados de teste: {X_test.shape}")
```

### 5.5 Casos de Uso PrÃ¡ticos

#### 5.5.1 RecomendaÃ§Ã£o de PreÃ§os

```python
"""
AnÃ¡lise: Qual o preÃ§o ideal para um livro de determinada categoria?
"""
import numpy as np

# Buscar livros de uma categoria
category = 'Fiction'
response = requests.get(
    f"{API_URL}/books",
    params={"category": category, "size": 1000}
)
fiction_books = pd.DataFrame(response.json())

# AnÃ¡lise estatÃ­stica
price_stats = fiction_books['price'].describe()
print(f"\nEstatÃ­sticas de preÃ§o para {category}:")
print(f"MÃ©dia: Â£{price_stats['mean']:.2f}")
print(f"Mediana: Â£{price_stats['50%']:.2f}")
print(f"Percentil 75: Â£{price_stats['75%']:.2f}")

# CorrelaÃ§Ã£o rating x preÃ§o
correlation = fiction_books[['price', 'rating']].corr()
print(f"\nCorrelaÃ§Ã£o preÃ§o x rating: {correlation.iloc[0,1]:.2f}")
```

#### 5.5.2 AnÃ¡lise de Estoque

```python
"""
AnÃ¡lise: Categorias com maior risco de falta de estoque
"""
# Buscar estatÃ­sticas por categoria
response = requests.get(f"{API_URL}/stats/categories")
category_stats = response.json()

# Calcular mÃ©dia de estoque por categoria
stock_by_category = pd.DataFrame([
    {
        'category': cat,
        'avg_stock': stats['average_stock'],
        'total_books': stats['total_books']
    }
    for cat, stats in category_stats.items()
])

# Ordenar por menor estoque mÃ©dio
low_stock = stock_by_category.sort_values('avg_stock').head(10)
print("Categorias com menor estoque mÃ©dio:")
print(low_stock)
```

#### 5.5.3 SegmentaÃ§Ã£o de Clientes

```python
"""
AnÃ¡lise: Agrupar livros por perfil de cliente
"""
from sklearn.cluster import KMeans

# Preparar features
X = books[['price', 'rating']].fillna(0)

# Aplicar K-means
kmeans = KMeans(n_clusters=3, random_state=42)
books['segment'] = kmeans.fit_predict(X)

# Analisar segmentos
segment_analysis = books.groupby('segment').agg({
    'price': ['mean', 'min', 'max'],
    'rating': 'mean',
    'id': 'count'
})

print("\nSegmentos de livros:")
print(segment_analysis)

# Nomear segmentos
segment_names = {
    0: 'EconÃ´mico',
    1: 'Premium',
    2: 'Popular'
}
books['segment_name'] = books['segment'].map(segment_names)
```

---

## 6. Plano de IntegraÃ§Ã£o com Modelos de ML

### 6.1 Arquitetura ML Proposta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DADOS (API)                        â”‚
â”‚  GET /api/v1/books â†’ DataFrame                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FEATURE ENGINEERING                    â”‚
â”‚  â€¢ Encoding de categorias                           â”‚
â”‚  â€¢ NormalizaÃ§Ã£o de preÃ§os                           â”‚
â”‚  â€¢ Features derivadas                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TREINAMENTO DE MODELO                  â”‚
â”‚  â€¢ Train/Test Split                                 â”‚
â”‚  â€¢ ValidaÃ§Ã£o Cruzada                                â”‚
â”‚  â€¢ OtimizaÃ§Ã£o de HiperparÃ¢metros                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODELO TREINADO                        â”‚
â”‚  â€¢ Salvar como pickle/joblib                        â”‚
â”‚  â€¢ Versionamento do modelo                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DEPLOY DO MODELO                       â”‚
â”‚  â€¢ Endpoint /api/v1/ml/predict                      â”‚
â”‚  â€¢ Carregamento do modelo na inicializaÃ§Ã£o          â”‚
â”‚  â€¢ InferÃªncia em tempo real                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Casos de Uso de ML

#### 6.2.1 Sistema de RecomendaÃ§Ã£o

**Objetivo:** Recomendar livros similares baseado em caracterÃ­sticas

```python
# models/recommendation.py

from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
import numpy as np

class BookRecommender:
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.model = None
        self.scaler = StandardScaler()
        self.books_data = None
        
    def fit(self):
        """Treina o modelo de recomendaÃ§Ã£o"""
        # Carregar dados
        response = requests.get(f"{self.api_url}/books", params={"size": 10000})
        self.books_data = pd.DataFrame(response.json())
        
        # Features para similaridade
        features = ['price', 'rating', 'category_encoded', 'stock']
        
        # Encoding
        le = LabelEncoder()
        self.books_data['category_encoded'] = le.fit_transform(
            self.books_data['category']
        )
        
        # NormalizaÃ§Ã£o
        X = self.books_data[features].fillna(0)
        X_scaled = self.scaler.fit_transform(X)
        
        # Treinar KNN
        self.model = NearestNeighbors(n_neighbors=6, metric='cosine')
        self.model.fit(X_scaled)
        
    def recommend(self, book_id: int, n: int = 5):
        """Recomenda N livros similares"""
        # Encontrar Ã­ndice do livro
        idx = self.books_data[self.books_data['id'] == book_id].index[0]
        
        # Encontrar vizinhos mais prÃ³ximos
        distances, indices = self.model.kneighbors(
            self.scaler.transform([self.books_data.iloc[idx][features]]),
            n_neighbors=n+1
        )
        
        # Retornar livros similares (exceto o prÃ³prio)
        recommendations = self.books_data.iloc[indices[0][1:]].copy()
        recommendations['similarity_score'] = 1 - distances[0][1:]
        
        return recommendations[['id', 'title', 'category', 'price', 'similarity_score']]

# Uso
recommender = BookRecommender("http://localhost:8000/api/v1")
recommender.fit()
similar_books = recommender.recommend(book_id=1, n=5)
print(similar_books)
```

#### 6.2.2 PrediÃ§Ã£o de Rating

**Objetivo:** Prever a avaliaÃ§Ã£o de um livro baseado em suas caracterÃ­sticas

```python
# models/rating_predictor.py

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score

class RatingPredictor:
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        self.feature_names = []
        
    def prepare_features(self, df: pd.DataFrame):
        """Prepara features para o modelo"""
        # Features numÃ©ricas
        df['price_range'] = pd.cut(df['price'], bins=5, labels=False)
        df['stock_level'] = pd.cut(df['stock'], bins=3, labels=False)
        
        # Encoding de categoria
        le = LabelEncoder()
        df['category_encoded'] = le.fit_transform(df['category'])
        
        # Features finais
        self.feature_names = ['price', 'stock', 'category_encoded', 
                             'price_range', 'stock_level']
        
        return df[self.feature_names].fillna(0)
    
    def train(self):
        """Treina o modelo"""
        # Carregar dados
        response = requests.get(f"{self.api_url}/books", params={"size": 10000})
        df = pd.DataFrame(response.json())
        
        # Preparar features e target
        X = self.prepare_features(df)
        y = df['rating']
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Treinar
        self.model.fit(X_train, y_train)
        
        # Avaliar
        y_pred = self.model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        print(f"RMSE: {rmse:.2f}")
        print(f"RÂ²: {r2:.2f}")
        
        # Feature importance
        importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nImportÃ¢ncia das features:")
        print(importance)
        
        return self.model
    
    def predict(self, book_data: dict):
        """Prediz rating para um novo livro"""
        df = pd.DataFrame([book_data])
        X = self.prepare_features(df)
        prediction = self.model.predict(X)[0]
        return round(prediction)

# Uso
predictor = RatingPredictor("http://localhost:8000/api/v1")
predictor.train()

# Prever rating de novo livro
new_book = {
    'price': 25.99,
    'stock': 15,
    'category': 'Fiction'
}
predicted_rating = predictor.predict(new_book)
print(f"Rating previsto: {predicted_rating}")
```

#### 6.2.3 ClassificaÃ§Ã£o de Categorias

**Objetivo:** Classificar automaticamente livros em categorias baseado no tÃ­tulo e descriÃ§Ã£o

```python
# models/category_classifier.py

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

class CategoryClassifier:
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),
            ('clf', MultinomialNB())
        ])
        
    def train(self):
        """Treina classificador de categorias"""
        # Carregar dados
        response = requests.get(f"{self.api_url}/books", params={"size": 10000})
        df = pd.DataFrame(response.json())
        
        # Combinar tÃ­tulo e descriÃ§Ã£o
        df['text'] = df['title'] + ' ' + df['description'].fillna('')
        
        # Preparar dados
        X = df['text']
        y = df['category']
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Treinar
        self.pipeline.fit(X_train, y_train)
        
        # Avaliar
        accuracy = self.pipeline.score(X_test, y_test)
        print(f"AcurÃ¡cia: {accuracy:.2%}")
        
        return self.pipeline
    
    def predict(self, title: str, description: str = ""):
        """Prediz categoria de um livro"""
        text = f"{title} {description}"
        prediction = self.pipeline.predict([text])[0]
        probas = self.pipeline.predict_proba([text])[0]
        
        # Top 3 categorias mais provÃ¡veis
        top_indices = np.argsort(probas)[-3:][::-1]
        top_categories = [
            (self.pipeline.classes_[i], probas[i]) 
            for i in top_indices
        ]
        
        return {
            'predicted_category': prediction,
            'top_3_categories': top_categories
        }

# Uso
classifier = CategoryClassifier("http://localhost:8000/api/v1")
classifier.train()

result = classifier.predict(
    title="The Great Adventure",
    description="A thrilling journey through unknown lands"
)
print(result)
```

### 6.3 Endpoint ML na API

**Proposta de implementaÃ§Ã£o:**

```python
# api/routes/ml.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import joblib

router = APIRouter(prefix="/api/v1/ml", tags=["ğŸ¤– Machine Learning"])

# Carregar modelos treinados na inicializaÃ§Ã£o
recommender = joblib.load('models/recommender.pkl')
rating_predictor = joblib.load('models/rating_predictor.pkl')
category_classifier = joblib.load('models/category_classifier.pkl')

class RecommendationRequest(BaseModel):
    book_id: int
    n_recommendations: int = 5

class RatingPredictionRequest(BaseModel):
    price: float
    stock: int
    category: str

class CategoryPredictionRequest(BaseModel):
    title: str
    description: str = ""

@router.post("/recommend")
def get_recommendations(request: RecommendationRequest):
    """Recomenda livros similares"""
    try:
        recommendations = recommender.recommend(
            request.book_id,
            request.n_recommendations
        )
        return recommendations.to_dict('records')
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/predict-rating")
def predict_rating(request: RatingPredictionRequest):
    """Prediz rating de um livro"""
    try:
        rating = rating_predictor.predict(request.dict())
        return {"predicted_rating": rating}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/classify-category")
def classify_category(request: CategoryPredictionRequest):
    """Classifica categoria de um livro"""
    try:
        result = category_classifier.predict(
            request.title,
            request.description
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Adicionar ao main.py:
# app.include_router(ml.router)
```

### 6.4 Pipeline de Treinamento ContÃ­nuo

```python
# scripts/train_models.py

"""
Script para treinar todos os modelos de ML
Deve ser executado periodicamente para atualizar os modelos
"""

import joblib
from datetime import datetime
import os

def train_all_models(api_url: str):
    """Treina todos os modelos e salva"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    models_dir = 'models'
    os.makedirs(models_dir, exist_ok=True)
    
    print("=" * 50)
    print("TREINAMENTO DE MODELOS ML")
    print("=" * 50)
    
    # 1. Sistema de RecomendaÃ§Ã£o
    print("\n1. Treinando Sistema de RecomendaÃ§Ã£o...")
    recommender = BookRecommender(api_url)
    recommender.fit()
    joblib.dump(recommender, f'{models_dir}/recommender.pkl')
    print("âœ“ Salvo: recommender.pkl")
    
    # 2. Preditor de Rating
    print("\n2. Treinando Preditor de Rating...")
    rating_predictor = RatingPredictor(api_url)
    rating_predictor.train()
    joblib.dump(rating_predictor, f'{models_dir}/rating_predictor.pkl')
    print("âœ“ Salvo: rating_predictor.pkl")
    
    # 3. Classificador de Categorias
    print("\n3. Treinando Classificador de Categorias...")
    category_classifier = CategoryClassifier(api_url)
    category_classifier.train()
    joblib.dump(category_classifier, f'{models_dir}/category_classifier.pkl')
    print("âœ“ Salvo: category_classifier.pkl")
    
    # Salvar metadados
    metadata = {
        'trained_at': timestamp,
        'api_url': api_url,
        'models': ['recommender', 'rating_predictor', 'category_classifier']
    }
    joblib.dump(metadata, f'{models_dir}/metadata.pkl')
    
    print("\n" + "=" * 50)
    print("TREINAMENTO CONCLUÃDO!")
    print("=" * 50)

if __name__ == "__main__":
    train_all_models("http://localhost:8000/api/v1")
```

### 6.5 Fluxo Completo: Dados â†’ ML â†’ PrediÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. COLETA DE DADOS                                    â”‚
â”‚     â€¢ Scraper extrai dados do site                    â”‚
â”‚     â€¢ Dados salvos em CSV                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. PROCESSAMENTO                                      â”‚
â”‚     â€¢ Limpeza com process_data.py                      â”‚
â”‚     â€¢ NormalizaÃ§Ã£o de valores                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. API DISPONIBILIZA DADOS                            â”‚
â”‚     â€¢ FastAPI carrega CSV                              â”‚
â”‚     â€¢ Endpoints fornecem dados estruturados            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. TREINAMENTO DE MODELOS                             â”‚
â”‚     â€¢ Scripts consomem API                             â”‚
â”‚     â€¢ Feature engineering aplicado                     â”‚
â”‚     â€¢ Modelos treinados e salvos                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. DEPLOY DE MODELOS                                  â”‚
â”‚     â€¢ Modelos carregados na API                        â”‚
â”‚     â€¢ Novos endpoints ML criados                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. INFERÃŠNCIA                                         â”‚
â”‚     â€¢ Clientes consomem endpoints ML                   â”‚
â”‚     â€¢ PrediÃ§Ãµes em tempo real                          â”‚
â”‚     â€¢ RecomendaÃ§Ãµes personalizadas                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ConclusÃ£o

Este documento apresenta a arquitetura completa do projeto Books Web Scraping & API, desde a coleta de dados atÃ© as possibilidades de integraÃ§Ã£o com modelos de Machine Learning.

### Pontos Fortes da Arquitetura Atual

âœ… **Pipeline ETL Completo** - IngestÃ£o, processamento e exposiÃ§Ã£o de dados
âœ… **API REST Robusta** - FastAPI com documentaÃ§Ã£o automÃ¡tica
âœ… **Modular e Organizado** - SeparaÃ§Ã£o clara de responsabilidades
âœ… **FÃ¡cil Consumo** - Endpoints intuitivos e bem documentados
âœ… **Pronto para ML** - Dados estruturados e acessÃ­veis via API

### PrÃ³ximos Passos Recomendados

1. **Curto Prazo**
   - MigraÃ§Ã£o de CSV para PostgreSQL
   - ImplementaÃ§Ã£o de testes automatizados
   - CI/CD com GitHub Actions

2. **MÃ©dio Prazo**
   - Cache distribuÃ­do com Redis
   - ImplementaÃ§Ã£o dos modelos ML propostos
   - Monitoramento e observabilidade

3. **Longo Prazo**
   - OrquestraÃ§Ã£o com Airflow
   - Arquitetura de microserviÃ§os
   - Escalabilidade horizontal com Kubernetes

---

**Documento criado por:** VinÃ­cius Miranda
**Data:** 01/01/2025  
**VersÃ£o:** 1.0
